{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA2X42ijsDMPgmlP6O1OeT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2311cs020112-cell/Training/blob/main/nlp_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2qsUPAeMqMU",
        "outputId": "551f8a6c-48b3-4f88-b7c1-74fbb5807a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "text=\"I love NLP\"\n",
        "tokens=nltk.word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_5s4uETMt1I",
        "outputId": "59e67344-4126-435e-8949-b9d1d9156129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'love', 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58202f2",
        "outputId": "b7a85a5d-a9fb-4ed3-b52f-8e5787f20f79"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "example_text = \"This is an example sentence for tokenization.\"\n",
        "tokens = nltk.word_tokenize(example_text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wc2uL9b67v2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45f9cc06",
        "outputId": "67e07135-f681-4bdb-92ca-bb9e387dc679"
      },
      "source": [
        "import re\n",
        "\n",
        "example_text_norm = \"Hello, World! This is an Example.\"\n",
        "normalized_text = example_text_norm.lower()\n",
        "normalized_text = re.sub(r'[^a-z0-9\\s]', '', normalized_text)\n",
        "print(f\"Original text: {example_text_norm}\")\n",
        "print(f\"Normalized text: {normalized_text}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: Hello, World! This is an Example.\n",
            "Normalized text: hello world this is an example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEmfcIFC8zWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd65b214",
        "outputId": "021fcd9a-733a-4a64-e7dc-f3b73a7f48f4"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "example_text_stopwords = \"This is an example sentence, and it demonstrates stopword removal.\"\n",
        "word_tokens = word_tokenize(example_text_stopwords)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "print(f\"Original text: {example_text_stopwords}\")\n",
        "print(f\"Filtered words (no stopwords): {filtered_sentence}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is an example sentence, and it demonstrates stopword removal.\n",
            "Filtered words (no stopwords): ['example', 'sentence', ',', 'demonstrates', 'stopword', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "text = [\"this\", \"is\", \"a\", \"place \",\"where\",\"I\",\"found\", \"space\"]\n",
        "res = [word for word in text if word.lower() not in stop_words]\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDb3-pRADQMU",
        "outputId": "3b5d6979-7d98-4262-c03c-795de6a1c5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['place ', 'found', 'space']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc229f7",
        "outputId": "6a503e31-654e-414a-f635-d15413d260df"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over 10 lazy dogs\"\n",
        "normalized_sentence = sentence.lower()\n",
        "# Remove non-alphanumeric characters except spaces\n",
        "normalized_sentence = re.sub(r'[^a-z0-9\\s]', '', normalized_sentence)\n",
        "\n",
        "# 2. Tokenization\n",
        "words = word_tokenize(normalized_sentence)\n",
        "\n",
        "# 3. Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "cleaned_sentence = ' '.join(filtered_words)\n",
        "\n",
        "print(f\"Original sentence: '{sentence}'\")\n",
        "print(f\"Normalized sentence: '{normalized_sentence}'\")\n",
        "print(f\"Tokenized words: {words}\")\n",
        "print(f\"Cleaned sentence (after stopword removal): '{cleaned_sentence}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: 'The quick brown fox jumps over 10 lazy dogs'\n",
            "Normalized sentence: 'the quick brown fox jumps over 10 lazy dogs'\n",
            "Tokenized words: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', '10', 'lazy', 'dogs']\n",
            "Cleaned sentence (after stopword removal): 'quick brown fox jumps 10 lazy dogs'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text=\"The quick brown fox jumps over the 10 lazy dogs\"\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "MNfNPs6ac75u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "paragraph = \"Python is great for AI. It's easy to learn! Do you agree?\"\n",
        "word_t=\"unrealistic\"\n",
        "sentences = sent_tokenize(paragraph)\n",
        "word=word_tokenize(word_t)\n",
        "\n",
        "print(sentences)\n",
        "print(word)#not working"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziNuV_f3QGdW",
        "outputId": "7bf3f68d-e046-467b-a97e-61e880988d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python is great for AI.', \"It's easy to learn!\", 'Do you agree?']\n",
            "['unrealistic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "TcYNVF9QbnfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eq5pX2fljSGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}